{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "tJ3Pfw42CJOh",
        "dtJnMEDqI90m",
        "vVCQCXwppHwO",
        "obLAqj4w3JWg",
        "_IdZCtzn3fj2",
        "BgnbrcyQ_0zk",
        "Z1foZIiIJIZ2",
        "uyxHCb4rJMmK",
        "jRq99vvIJMdx",
        "lGjkq1QTNVrW",
        "KxPNdfgSagQc",
        "dkx6SKfCzJU5"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marcellobenedetti92/natural-language-processing-con-Python/blob/main/progetto_finale_codice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creare un sistema di Spam Detection"
      ],
      "metadata": {
        "id": "fBmtl2zkma4F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "L'azienda ProfessionAI vuole realizzare una libreria capace di fare analisi delle email ricevute.\n",
        "\n",
        "Nello specifico, il CEO ha richiesto di identificare le email di tipo SPAM sulle quali fare analisi contenutistiche.\n",
        "\n",
        "Il CTO nello specifico ti fornisce un dataset e ti chiede di:\n",
        "- Addestrare un classificatore per identificare SPAM\n",
        "- Individuare i Topic principali tra le email SPAM presenti nel dataset\n",
        "- Calcolare la distanza semantica tra i topics ottenuti, per dedurne l'eterogeneità.\n",
        "- Estrarre dalle mail NON SPAM le Organizzazioni presenti."
      ],
      "metadata": {
        "id": "nLt7Gn3DmX38"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Addestrare un classificatore per identificare SPAM"
      ],
      "metadata": {
        "id": "tJ3Pfw42CJOh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## /"
      ],
      "metadata": {
        "id": "dtJnMEDqI90m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "RxVBHqpZ3Rhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "URL = \"https://raw.githubusercontent.com/ProfAI/natural-language-processing/main/datasets/Verifica%20Finale%20-%20Spam%20Detection/\""
      ],
      "metadata": {
        "id": "TIFmVZxMTTaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcLl-Pdu0Q5m"
      },
      "outputs": [],
      "source": [
        "dataframe = pd.read_csv(URL+\"spam_dataset.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe.head()"
      ],
      "metadata": {
        "id": "qQ_ZyEpy07iO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe.shape"
      ],
      "metadata": {
        "id": "5J8oAp5W3Zgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe[\"label_num\"].isnull().sum(axis = 0)"
      ],
      "metadata": {
        "id": "7H7NIeoH_bLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# count the number of unique values in unnamed column\n",
        "dataframe[\"Unnamed: 0\"].size"
      ],
      "metadata": {
        "id": "XCEl1FhQBlP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# droppo la colonna popolata da valori unici in quanto informazioni non rilevanti ai fini del modello\n",
        "dataframe = dataframe.drop(\"Unnamed: 0\", axis=1)"
      ],
      "metadata": {
        "id": "ddCo2U53C7o7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe.head()"
      ],
      "metadata": {
        "id": "w5_MYipDDUXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset sbilanciato?\n",
        "from collections import Counter\n",
        "print(Counter(dataframe[\"label_num\"]))\n",
        "print(str(round(3672/5171*100))+\"%\")\n",
        "print(str(round(1499/5171*100))+\"%\")"
      ],
      "metadata": {
        "id": "OAp8HGfWDX3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "dataset leggermente sbilanciato,\n",
        "\n",
        "provare in seguito class_weight=\"balanced\" nella funzione di attivazione Regressione Logistica prima di eventuali under/overfitting"
      ],
      "metadata": {
        "id": "Y2Mf7dawmzys"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data cleaning"
      ],
      "metadata": {
        "id": "vVCQCXwppHwO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# importo la punteggiatura da rimuovere\n",
        "import string\n",
        "\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# per gestire numeri e spazi multipli\n",
        "import re\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# importo le stopwords in lingua inglese\n",
        "english_stopwords = stopwords.words(\"english\")\n",
        "\n",
        "# estendo le stopwords anche alla parola \"Subject\"\n",
        "english_stopwords.extend(['subject'])\n",
        "\n",
        "# importo il modello nlp\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "punctuation = set(string.punctuation)"
      ],
      "metadata": {
        "id": "VVTkVV1LtMs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data cleaning function\n",
        "\n",
        "def data_cleaner(sentence):\n",
        "    sentence = sentence.lower()\n",
        "    for c in string.punctuation:\n",
        "        sentence = sentence.replace(c, \" \")\n",
        "    document = nlp(sentence)\n",
        "    sentence = ' '.join(token.lemma_ for token in document)\n",
        "    sentence = ' '.join(word for word in sentence.split() if word not in english_stopwords)\n",
        "    sentence = re.sub('\\d', '', sentence)\n",
        "    \n",
        "    return sentence"
      ],
      "metadata": {
        "id": "uxw6WxKYRHOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = dataframe[\"text\"]\n",
        "X.shape"
      ],
      "metadata": {
        "id": "64uIPfKYvn5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = dataframe[\"label\"]\n",
        "y.shape"
      ],
      "metadata": {
        "id": "eFt29BW_w6P_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_cleaned = []\n",
        "for text in X:\n",
        "    X_cleaned.append(data_cleaner(text))"
      ],
      "metadata": {
        "id": "KrPikqGYxCCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_cleaned[0]"
      ],
      "metadata": {
        "id": "bVRtrJzUx-fa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Test Split"
      ],
      "metadata": {
        "id": "obLAqj4w3JWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_cleaned, y, test_size=.3)"
      ],
      "metadata": {
        "id": "fJnoxq85ytIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tf-idf Vectorizer"
      ],
      "metadata": {
        "id": "_IdZCtzn3fj2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# funzione\n",
        "\n",
        "def bow_tfidf(dataset, tfidf_vectorizer):\n",
        "    \n",
        "    if tfidf_vectorizer == None:\n",
        "        tfidf_vectorizer = TfidfVectorizer()\n",
        "        X = tfidf_vectorizer.fit_transform(dataset)\n",
        "    else:\n",
        "        X = tfidf_vectorizer.transform(dataset)\n",
        "        \n",
        "    return X.toarray(), tfidf_vectorizer"
      ],
      "metadata": {
        "id": "0nIFjzQa3Snx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, vectorizer = bow_tfidf(X_train, None)"
      ],
      "metadata": {
        "id": "fIxthj9r5vWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test, vectorizer = bow_tfidf(X_test, vectorizer)"
      ],
      "metadata": {
        "id": "IDiIJ_nQ5vOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "id": "wFWvtFoe_evy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelli"
      ],
      "metadata": {
        "id": "BgnbrcyQ_0zk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "kfbioPqoHhZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Regressione Logistica"
      ],
      "metadata": {
        "id": "Z1foZIiIJIZ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "lr = LogisticRegression()\n",
        "\n",
        "lr.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "D8UWZCYJJZp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_train = lr.predict(X_train)\n",
        "y_pred_test = lr.predict(X_test)\n",
        "print(classification_report(y_train, y_pred_train))\n",
        "print(classification_report(y_test, y_pred_test))"
      ],
      "metadata": {
        "id": "CXEc3-fUKDDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Regressione Logistica con class_weight=\"balanced\""
      ],
      "metadata": {
        "id": "uyxHCb4rJMmK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr_bal = LogisticRegression(class_weight= \"balanced\")\n",
        "\n",
        "lr_bal.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "PXAXV-f3JaBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_train = lr_bal.predict(X_train)\n",
        "y_pred_test = lr_bal.predict(X_test)\n",
        "print(classification_report(y_train, y_pred_train))\n",
        "print(classification_report(y_test, y_pred_test))"
      ],
      "metadata": {
        "id": "oe6leZOGLSDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MLP Classifier"
      ],
      "metadata": {
        "id": "jRq99vvIJMdx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# allo stato dell'arte la funzione di attivazione più utilizzata è logistic, quindi:\n",
        "clf = MLPClassifier(activation= \"logistic\",\n",
        "                    solver = \"adam\",\n",
        "                    max_iter = 100, # num max di iterazioni per la fase di train\n",
        "                    hidden_layer_sizes = (100,), # un solo layer\n",
        "                    tol = 0.005,\n",
        "                    verbose = True\n",
        "                   )\n",
        "\n",
        "clf.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "r_Sjh3bw_2Uj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_train = clf.predict(X_train)\n",
        "y_pred_test = clf.predict(X_test)\n",
        "print(classification_report(y_train, y_pred_train))\n",
        "print(classification_report(y_test, y_pred_test))"
      ],
      "metadata": {
        "id": "gG-Whj-FMZSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2) Individuare i Topic principali tra le email SPAM presenti nel dataset"
      ],
      "metadata": {
        "id": "lGjkq1QTNVrW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe = pd.read_csv(URL+\"spam_dataset.csv\")"
      ],
      "metadata": {
        "id": "UAa14pe5TrkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe_spam = dataframe[dataframe[\"label\"]==\"spam\"]"
      ],
      "metadata": {
        "id": "D62YCpqyNXus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe_spam.head()"
      ],
      "metadata": {
        "id": "f3sWsTdYUlYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim"
      ],
      "metadata": {
        "id": "QqsWHZc7UnW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.utils import simple_preprocess"
      ],
      "metadata": {
        "id": "it6y7paiUvc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "ohwNyIGgVbil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words(\"english\")"
      ],
      "metadata": {
        "id": "CeSKSKbnVhEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = dataframe_spam[\"text\"]"
      ],
      "metadata": {
        "id": "ovB8sf1OV7DY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents"
      ],
      "metadata": {
        "id": "EAChy3SAXS5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sent_to_words(items):\n",
        "    for item in items:\n",
        "        yield(simple_preprocess(item, deacc=True))\n",
        "        # yield è un tipo di return speciale, che restituisce una lista\n",
        "        # che contiene tutti i return del ciclo\n",
        "        # deacc a True rimuove la punteggiatura\n",
        "        \n",
        "def remove_stopwords(texts):\n",
        "    return [[word for word in words if word not in stop_words and len(word) >=5 and word != \"subject\"] for words in texts]"
      ],
      "metadata": {
        "id": "Z3Hs_w1_XV1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_words = list(sent_to_words(documents))"
      ],
      "metadata": {
        "id": "vyyX6dnIXi47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_words"
      ],
      "metadata": {
        "id": "95bcKXlGXo62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_words = remove_stopwords(data_words)"
      ],
      "metadata": {
        "id": "GkbdMdZEXrTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_words"
      ],
      "metadata": {
        "id": "SuOfLl5iXyrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.corpora as corpora"
      ],
      "metadata": {
        "id": "dJIFMgn8Xzvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creo il dizionario passando la lista generata dallo step precedente\n",
        "id2word = corpora.Dictionary(data_words)"
      ],
      "metadata": {
        "id": "q9Z4ZH2XX5zP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vettorizzo usando il metodo doc2bow\n",
        "corpus = [id2word.doc2bow(text) for text in data_words]"
      ],
      "metadata": {
        "id": "3_TXHMKzX983"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus"
      ],
      "metadata": {
        "id": "dkgRZNECYBod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LDA con Gensim"
      ],
      "metadata": {
        "id": "_V_kaY4rYUgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# per print più formattati importo:\n",
        "from pprint import pprint"
      ],
      "metadata": {
        "id": "cfyMzDkqYDRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_topics = 3"
      ],
      "metadata": {
        "id": "5OIuhR4iYedZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
        "                                       id2word=id2word,\n",
        "                                       num_topics=num_topics,\n",
        "                                       passes=3\n",
        "                                      )"
      ],
      "metadata": {
        "id": "I3KhcZAnYnnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(lda_model.print_topics())"
      ],
      "metadata": {
        "id": "eRi1h3tKYySU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_lda = lda_model[corpus]"
      ],
      "metadata": {
        "id": "ipy-FMphYywY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3) Calcolare la distanza semantica tra i topic ottenuti, per dedurne l'eterogeneità"
      ],
      "metadata": {
        "id": "KxPNdfgSagQc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec"
      ],
      "metadata": {
        "id": "-iGOTnB0ak-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader\n",
        "# con cui scaricare il modello pre-trained di word2vec:\n",
        "glove_vectors = gensim.downloader.load(\"glove-wiki-gigaword-300\")"
      ],
      "metadata": {
        "id": "D7wM6EYVdEDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importo spatial per il calcolo della similarità semantica:\n",
        "from scipy import spatial"
      ],
      "metadata": {
        "id": "f28QCwQZdL5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "bT1KYnYoh1BN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# funzione di average\n",
        "\n",
        "def avg_vector(sentence):\n",
        "    to_remove = 0\n",
        "    vector = np.zeros(300)\n",
        "    for word in sentence:\n",
        "        if word in glove_vectors.key_to_index.keys():\n",
        "            vector += glove_vectors.get_vector(word)\n",
        "        else:\n",
        "            to_remove += 1\n",
        "    if len(sentence) == to_remove:\n",
        "        return np.zeros(300)\n",
        "        \n",
        "    return vector/(len(sentence)-to_remove)"
      ],
      "metadata": {
        "id": "f_WRPuw5eXLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for t in lda_model.print_topics():\n",
        "    with open (\"topic_file.txt\", \"a+\", encoding = \"utf8\") as topic_file:\n",
        "        t = str(t)+\"\\n\"\n",
        "        topic_rec = topic_file.write(t)"
      ],
      "metadata": {
        "id": "iiwx9BjTmCZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"topic_file.txt\", \"r+\", encoding = \"utf8\") as topic_file:\n",
        "                topic_rec = topic_file.readlines()\n",
        "        \n",
        "emp_list = topic_rec\n",
        "\n",
        "# Assegnazione delle nuvole di parole ai topic \n",
        "t_1 = emp_list[0]\n",
        "t_2 = emp_list[1]\n",
        "t_3 = emp_list[-1]"
      ],
      "metadata": {
        "id": "7QOwKJSqmDOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# topic cleaning\n",
        "\n",
        "topic_words_cleaned = list(sent_to_words(emp_list))\n",
        "topic_words_cleaned = remove_stopwords(topic_words_cleaned)\n",
        "\n",
        "print(topic_words_cleaned[0])\n",
        "print(topic_words_cleaned[1])\n",
        "print(topic_words_cleaned[2])"
      ],
      "metadata": {
        "id": "CyfMsaJin_xy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_list_vect = []\n",
        "for topic in topic_words_cleaned:\n",
        "    topic_list_vect.append(avg_vector(topic))\n",
        "\n",
        "topic_list_vect"
      ],
      "metadata": {
        "id": "UgY_DtPloCg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_1_2 = 1 - spatial.distance.cosine(topic_list_vect[0], topic_list_vect[1])\n",
        "topic_1_3 = 1 - spatial.distance.cosine(topic_list_vect[0], topic_list_vect[2])\n",
        "topic_2_3 = 1 - spatial.distance.cosine(topic_list_vect[1], topic_list_vect[2])\n",
        "\n",
        "print(f\"Cosine similarity topic_1_2 : {topic_1_2}\")\n",
        "print(f\"Cosine similarity topic_1_3 : {topic_1_3}\")\n",
        "print(f\"Cosine similarity topic_2_3 : {topic_2_3}\")"
      ],
      "metadata": {
        "id": "bOXJqdiRuYSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4) Estrarre dalle mail NON SPAM le Organizzazioni presenti"
      ],
      "metadata": {
        "id": "dkx6SKfCzJU5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "l = [\"ham\",]\n",
        "\n",
        "dataset_ham = dataframe[dataframe[\"label\"].isin(l)]\n",
        "dataset_ham.drop([\"Unnamed: 0\", \"label_num\" ], axis=1, inplace=True)\n",
        "\n",
        "dataset_ham.head()"
      ],
      "metadata": {
        "id": "YjuxhB4nu9DQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_ham = dataset_ham[\"text\"]\n",
        "X_ham.head()"
      ],
      "metadata": {
        "id": "MMjh9Q7pzSF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_ham_cleaned = []\n",
        "for text in X_ham:\n",
        "    X_ham_cleaned.append(data_cleaner(text))"
      ],
      "metadata": {
        "id": "a4x8v_ZPzcjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_ham_cleaned"
      ],
      "metadata": {
        "id": "8T64Ra9Pzf9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def org_entitie(sentence):\n",
        "    to_return = {\"ORG\":[]}\n",
        "    doc = nlp(sentence)\n",
        "    \n",
        "    for token in doc:\n",
        "        if str(token.ent_type_) == \"ORG\":\n",
        "            to_return[str(token.ent_type_)].append(str(token))\n",
        "\n",
        "    return to_return"
      ],
      "metadata": {
        "id": "iI_S_AmFziiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in X_ham_cleaned:\n",
        "    \n",
        "    org_dict = (org_entitie(sentence))\n",
        "    if org_dict != {\"ORG\":[]}:\n",
        "        \n",
        "                # Salvare le ORG su txt\n",
        "        \n",
        "        org_dict = str(org_dict)\n",
        "        \n",
        "        with open (\"org_file.txt\", \"a+\", encoding = \"utf8\") as org:\n",
        "            org_rec = org.write(org_dict+\"\\n\")\n",
        "        \n",
        "        print(org_dict)"
      ],
      "metadata": {
        "id": "6Ssdgajjzlot"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}